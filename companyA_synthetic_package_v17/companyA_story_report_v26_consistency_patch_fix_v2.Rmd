---
title: "Company A — Practice Report (Lean Production + Optimization)"
output:
  html_document:
    toc: true
    toc_depth: 4
    theme: flatly
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, message = FALSE, warning = FALSE
)

suppressPackageStartupMessages({
  library(tidyverse)
  library(lubridate)
  library(scales)
})

ensure_pkg <- function(p){
  if (!requireNamespace(p, quietly = TRUE)) install.packages(p, repos = "https://cloud.r-project.org")
}
ensure_pkg("highcharter")
ensure_pkg("lpSolve")
ensure_pkg("DiagrammeR")

library(highcharter)
library(lpSolve)
library(DiagrammeR)

options(highcharter.theme = hc_theme_flat())
hc_lang <- list(thousandsSep = ",", decimalPoint = ".")

# Highcharter version compatibility: some versions do not have hc_lang()
hc_lang_safe <- function(h, lang_list){
  # If hc_lang exists, use it; otherwise try hc_opts(lang=); else no-op.
  if (exists("hc_lang", where=asNamespace("highcharter"), inherits=FALSE)) {
    return(highcharter::hc_lang(h, lang_list))
  }
  if (exists("hc_opts", where=asNamespace("highcharter"), inherits=FALSE)) {
    return(highcharter::hc_opts(h, lang = lang_list))
  }
  h
}


# ---- run-from-anywhere path helper ----
get_script_dir <- function(){
  cmdArgs <- commandArgs(trailingOnly = FALSE)
  fileArgName <- "--file="
  m <- grep(fileArgName, cmdArgs)
  if (length(m) > 0) return(dirname(normalizePath(sub(fileArgName, "", cmdArgs[m]), winslash="/")))
  if (!is.null(sys.frames()[[1]]$ofile)) return(dirname(normalizePath(sys.frames()[[1]]$ofile, winslash="/")))
  if (requireNamespace("rstudioapi", quietly = TRUE) && rstudioapi::isAvailable()) {
    p <- rstudioapi::getActiveDocumentContext()$path
    if (nzchar(p)) return(dirname(normalizePath(p, winslash="/")))
  }
  normalizePath(getwd(), winslash="/")
}
PROJECT_ROOT <- get_script_dir()
base_dir <- file.path(PROJECT_ROOT, "companyA_synthetic")
dir.create(file.path(base_dir, "analysis_outputs_r"), showWarnings = FALSE, recursive = TRUE)
dir.create(file.path(base_dir, "optimization_outputs_r"), showWarnings = FALSE, recursive = TRUE)

COL_BEFORE <- "#2C3E50"
COL_AFTER  <- "#18BC9C"
COL_WARN   <- "#F39C12"
COL_BAD    <- "#E74C3C"

# Helper: highcharter time series
hc_timeseries <- function(df, x, y, group=NULL, title="", ytitle="", subtitle=""){
  df <- df %>% arrange({{x}})
  h <- highchart() %>%
    hc_chart(zoomType="x") %>%
    hc_title(text = title) %>%
    hc_subtitle(text = subtitle) %>%
    hc_xAxis(type="datetime") %>%
    hc_yAxis(title = list(text=ytitle)) %>%
    hc_tooltip(shared=TRUE, crosshairs=TRUE) %>% hc_lang_safe(hc_lang)
  if (is.null(group)){
    h <- h %>% hc_add_series(df, "line", hcaes(x={{x}}, y={{y}}))
  } else {
    h <- h %>% hc_add_series_list(
      df %>%
        group_by({{group}}) %>%
        group_map(~ list(
          name = as.character((unique(.x %>% pull({{group}}))[1])),
          type = "line",
          data = .x %>% transmute(x=datetime_to_timestamp({{x}}), y={{y}}) %>% list_parse2()
        ))
    )
  }
  h
}

datetime_to_timestamp <- function(x){
  as.numeric(as.POSIXct(x)) * 1000
}

```
## 7. Practical Application (Practice)
### 7.1 How we apply Lean thinking in this practice
We take the Lean ideas from the outline, waste, flow, pull, and improvement mindset, then we apply it to one real-looking operation situation. We read the charts in order to define what problem is, then we connect signal together, and only after that we propose solution. In this dataset, we focus on flow stability, constraint behavior, quality loop, and inventory timing, because these are the common driver that make operation look busy but customer still wait.

### 7.1.1 Story map
```{r story-map}
# Story map = how to read (top-down) and how to trace causes (bottom-up).
# Safe DiagrammeR DOT string (no parse issues).

ensure_pkg <- function(p){
  if (!requireNamespace(p, quietly = TRUE)) install.packages(p, repos = "https://cloud.r-project.org")
}
ensure_pkg("DiagrammeR")
library(DiagrammeR)

dot <- paste0(
'digraph G {
',
'  graph [rankdir=TB, bgcolor="white"];
',
'  node [shape=box, style="rounded,filled", color="#bdc3c7", fillcolor="#f8f9fa", fontname="Arial"];
',
'  edge [color="#7f8c8d"];
',
'  A [label="Demand (variability)\nspike/seasionality"];
',
'  B [label="Capacity context\n(machine-hours)"];
',
'  C [label="Flow symptom\nWaiting + WIP"];
',
'  D [label="Bottleneck evidence\n(utilization heatmap)"];
',
'  E [label="Quality loss\n(reject + rework + Pareto)"];
',
'  F [label="Inventory imbalance\n(on-hand vs backorder)"];
',
'  G [label="Solution\nLean countermeasure"];
',
'  H [label="Optimization\nProduction LP + Network flow"];
',
'  I [label="Impact\nBefore vs After"];
',
'  A -> B -> C -> D;
',
'  D -> E;
',
'  E -> C;
',
'  C -> F;
',
'  D -> G -> H -> I;
',
'  F -> I;
',
'}
'
)

grViz(dot)
```
### 7.1.2 Load data
```{r load-raw}
load_scn <- function(scn){
  d <- file.path(base_dir, scn)
  list(
    prod = read_csv(file.path(d,"production_events.csv"), show_col_types = FALSE) %>%
      mutate(step_start_time = ymd_hms(step_start_time),
             step_end_time   = ymd_hms(step_end_time)),
    qi   = read_csv(file.path(d,"quality_inspections.csv"), show_col_types = FALSE) %>%
      mutate(inspection_time = ymd_hms(inspection_time)),
    inv  = read_csv(file.path(d,"inventory_snapshots.csv"), show_col_types = FALSE) %>%
      mutate(snapshot_date = ymd(snapshot_date)),
    dem  = read_csv(file.path(d,"demand_orders.csv"), show_col_types = FALSE) %>%
      mutate(order_date = ymd(order_date),
             due_date   = ymd(due_date)),
    sh   = read_csv(file.path(d,"shipments.csv"), show_col_types = FALSE) %>%
      mutate(ship_date = ymd(ship_date),
             planned_delivery_date = ymd(planned_delivery_date),
             actual_delivery_date  = ymd(actual_delivery_date)),
    lines = read_csv(file.path(d,"production_lines.csv"), show_col_types = FALSE),
    skus  = read_csv(file.path(d,"skus.csv"), show_col_types = FALSE),
    lanes = read_csv(file.path(d,"transportation_lanes.csv"), show_col_types = FALSE),
    customers = read_csv(file.path(d,"customers.csv"), show_col_types = FALSE),
    scenario = scn
  )
}

B <- load_scn("before")
A <- load_scn("after")
```
### 7.1.3 What we want to do here
We want to apply all of knowledge which we learn to use in realiaty, not only theory, like how we would do in a factory. We start from demand pressure, then we follow the flow inside the value stream, we check waiting and bottleneck, we confirm hidden rework loops, then we look at WIP and inventory pain. After we define the real problem, we apply Lean solution and we use LP/network flow to plan in a more stable way

```{r master-data}

# Build a "master" dataset so we can answer: data input come from where?
# We create (1) event-level master, (2) weekly SKU-location master.

# (1) Event-level master (production events enriched)
orders_small <- B$dem %>%
  select(order_id, customer_id, sku_id, order_date, due_date, demand_qty, penalty_cost_late, priority)

master_events_before <- B$prod %>%
  left_join(B$skus, by="sku_id") %>%
  left_join(B$lines, by=c("factory_id","production_line_id")) %>%
  left_join(orders_small, by=c("sales_order_id"="order_id","sku_id")) %>%
  mutate(week = as.Date(floor_date(step_end_time, "week", week_start = 1)))

# (2) Weekly master (SKU + location), connect demand, throughput, inventory, shipments
wk_demand_sku <- B$dem %>%
  mutate(week = as.Date(floor_date(due_date, "week", week_start = 1))) %>%
  group_by(week, sku_id) %>%
  summarise(demand_qty = sum(demand_qty), .groups="drop")

wk_prod_sku <- B$prod %>%
  filter(operation_step == "PACK") %>%
  mutate(week = as.Date(floor_date(step_end_time, "week", week_start = 1))) %>%
  group_by(week, sku_id) %>%
  summarise(throughput_qty = sum(produced_qty), .groups="drop")

wk_ship_sku_loc <- B$sh %>%
  mutate(week = as.Date(floor_date(ship_date, "week", week_start = 1)),
         location_id = to_node,
         late_days = as.numeric(difftime(actual_delivery_date, planned_delivery_date, units="days")),
         is_late = late_days > 0) %>%
  group_by(week, sku_id, location_id) %>%
  summarise(shipped_qty = sum(shipped_qty),
            late_ship_qty = sum(if_else(is_late, shipped_qty, 0)),
            freight_cost = sum(freight_cost),
            .groups="drop")

wk_inv_sku_loc <- B$inv %>%
  mutate(week = as.Date(floor_date(snapshot_date, "week", week_start = 1))) %>%
  group_by(week, sku_id, location_id) %>%
  summarise(on_hand_qty = sum(on_hand_qty),
            backorder_qty = sum(backorder_qty),
            reserved_qty = sum(reserved_qty),
            .groups="drop")

master_weekly_before <- wk_inv_sku_loc %>%
  full_join(wk_ship_sku_loc, by=c("week","sku_id","location_id")) %>%
  full_join(wk_demand_sku, by=c("week","sku_id")) %>%
  full_join(wk_prod_sku, by=c("week","sku_id")) %>%
  replace_na(list(on_hand_qty=0, backorder_qty=0, reserved_qty=0,
                  shipped_qty=0, late_ship_qty=0, freight_cost=0,
                  demand_qty=0, throughput_qty=0)) %>%
  left_join(B$skus, by="sku_id")

# Save for sharing / audit trail (so we can show input data used in charts)
out_dir <- file.path(PROJECT_ROOT, "companyA_synthetic", "analysis_outputs_r")
dir.create(out_dir, showWarnings=FALSE, recursive=TRUE)

write_csv(master_events_before, file.path(out_dir, "master_events_before.csv"))
write_csv(master_weekly_before, file.path(out_dir, "master_weekly_sku_location_before.csv"))

```
### 7.1.4 Thing to keep in mind
Eventhough this case is based on synthetic data generation, it is not a real company data. But the goal of us is to practice the logic, how we read signals, define problem and decide what action and what model to solve
```{r metrics-before}

# Simple summary numbers we will reference in the narrative (BEFORE)
# Keep numbers simple and robust, we use same definitions as the charts.

# Demand per week (due week)
demand_wk <- B$dem %>%
  mutate(week = as.Date(floor_date(due_date, "week", week_start = 1))) %>%
  group_by(week) %>%
  summarise(demand_qty = sum(demand_qty), .groups="drop") %>%
  arrange(week)

demand_avg <- mean(demand_wk$demand_qty)
demand_max <- max(demand_wk$demand_qty)

# Throughput per week = PACK output (finished)
throughput_wk <- B$prod %>%
  filter(operation_step == "PACK") %>%
  mutate(week = as.Date(floor_date(step_end_time, "week", week_start = 1))) %>%
  group_by(week) %>%
  summarise(throughput_qty = sum(produced_qty), .groups="drop") %>%
  arrange(week)

gap_wk <- full_join(throughput_wk, demand_wk, by="week") %>%
  replace_na(list(throughput_qty=0, demand_qty=0)) %>%
  mutate(gap = throughput_qty - demand_qty) %>%
  arrange(week)

gap_min <- min(gap_wk$gap)
gap_max <- max(gap_wk$gap)
shortage_weeks <- sum(gap_wk$gap < 0)

# Capacity proxy (units/week) from line capacity and observed mh_per_unit
mh_per_unit <- B$prod %>% filter(produced_qty>0) %>% summarise(v = sum(machine_hours)/sum(produced_qty)) %>% pull(v)
cap_mh_week <- B$lines %>% summarise(v = sum(daily_capacity_machine_hours) * 6) %>% pull(v)
cap_units_week <- cap_mh_week / mh_per_unit

# Waiting & cycle by step (event-level, step-to-step gaps within same WO)
pe <- B$prod %>%
  arrange(work_order_id, step_start_time) %>%
  group_by(work_order_id) %>%
  mutate(prev_end = lag(step_end_time),
         wait_min = as.numeric(difftime(step_start_time, prev_end, units="mins"))) %>%
  ungroup() %>%
  mutate(wait_min = if_else(is.na(wait_min) | wait_min < 0, NA_real_, wait_min),
         cycle_min = as.numeric(difftime(step_end_time, step_start_time, units="mins")))

step_wait <- pe %>%
  group_by(operation_step) %>%
  summarise(avg_wait = mean(wait_min, na.rm=TRUE),
            p95_wait = quantile(wait_min, 0.95, na.rm=TRUE),
            avg_cycle = mean(cycle_min, na.rm=TRUE),
            .groups="drop") %>%
  arrange(desc(avg_wait))

top_wait_step <- step_wait$operation_step[1]
top_wait_avg  <- step_wait$avg_wait[1]
top_wait_p95  <- step_wait$p95_wait[1]

# WIP (count of in-progress items at mid-day)
days <- seq.Date(as.Date(min(B$prod$step_start_time)), as.Date(max(B$prod$step_end_time)), by="day")
mid_ts <- as.POSIXct(days) + hours(12)

wip_total <- map_int(mid_ts, function(tt){
  sum(B$prod$step_start_time <= tt & B$prod$step_end_time > tt, na.rm=TRUE)
})

wip_avg <- mean(wip_total)
wip_peak <- max(wip_total)

# Quality signal (weekly rates)
qwk <- B$prod %>%
  mutate(week = as.Date(floor_date(step_start_time, "week", week_start = 1))) %>%
  group_by(week) %>%
  summarise(produced = sum(produced_qty),
            rejected = sum(rejected_qty),
            rework = sum(if_else(rework_qty>0, rework_qty, produced_qty)),
            .groups="drop") %>%
  mutate(reject_rate = rejected/produced,
         rework_rate = rework/produced)

reject_avg <- mean(qwk$reject_rate, na.rm=TRUE)
rework_avg <- mean(qwk$rework_rate, na.rm=TRUE)

reject_peak <- max(qwk$reject_rate, na.rm=TRUE)
rework_peak <- max(qwk$rework_rate, na.rm=TRUE)

# Pareto defects (inspection FAIL only)
pareto <- B$qi %>%
  filter(result == "FAIL") %>%
  mutate(defect_code = replace_na(defect_code, "UNKNOWN")) %>%
  group_by(defect_code) %>%
  summarise(defects = sum(defects_found_qty), .groups="drop") %>%
  arrange(desc(defects))

top_defect <- pareto$defect_code[1]
top_defect_share <- pareto$defects[1] / sum(pareto$defects)

top5_share <- sum(head(pareto$defects, 5)) / sum(pareto$defects)

# Inventory imbalance (weekly totals)
inv_wk <- B$inv %>%
  mutate(week = as.Date(floor_date(snapshot_date, "week", week_start = 1))) %>%
  group_by(week) %>%
  summarise(on_hand = sum(on_hand_qty),
            backorder = sum(backorder_qty), .groups="drop") %>%
  arrange(week)

backorder_max <- max(inv_wk$backorder)
onhand_max <- max(inv_wk$on_hand)

both_high_n <- sum(inv_wk$on_hand >= quantile(inv_wk$on_hand, 0.75) &
                   inv_wk$backorder >= quantile(inv_wk$backorder, 0.75))

# Delivery lateness
late <- B$sh %>%
  mutate(late_days = as.numeric(difftime(actual_delivery_date, planned_delivery_date, units="days")),
         is_late = late_days > 0)

late_rate <- mean(late$is_late)
late_days_p95 <- quantile(late$late_days[late$late_days>0], 0.95, na.rm=TRUE)

M <- list(
  demand_avg=demand_avg, demand_max=demand_max,
  cap_units_week=cap_units_week,
  gap_min=gap_min, gap_max=gap_max, shortage_weeks=shortage_weeks,
  top_wait_step=top_wait_step, top_wait_avg=top_wait_avg, top_wait_p95=top_wait_p95,
  wip_avg=wip_avg, wip_peak=wip_peak,
  reject_avg=reject_avg, rework_avg=rework_avg,
  reject_peak=reject_peak, rework_peak=rework_peak,
  top_defect=top_defect, top_defect_share=top_defect_share, top5_share=top5_share,
  backorder_max=backorder_max, onhand_max=onhand_max, both_high_n=both_high_n,
  late_rate=late_rate, late_days_p95=late_days_p95
)

```
## 7.2 Demand shape with capacity context
The demand line show variability and some spike. Demand itself is not waste, but it become risk when the system can not absorb it. In the dataset, average weekly demand is about `r round(M$demand_avg)` unit, and peak week reach around `r round(M$demand_max)` unit. Our capacity proxy is around `r round(M$cap_units_week)` unit per week, so the real issue is not only total capacity on paper. The issue is how the flow absorb variability, how stable the release is, and whether the mix is correct.

```{r demand-shape}
# Demand variability itself is not waste.
# Waste happens when the system cannot absorb variability → waiting, overtime, expediting, inventory swings.

dem_wk <- B$dem %>%
  mutate(week = as.Date(floor_date(due_date, "week", week_start=1))) %>%
  group_by(week) %>%
  summarise(demand_qty = sum(demand_qty), orders=n(), .groups="drop") %>%
  arrange(week)

# ---- Reference lines (context) ----
avg_demand <- mean(dem_wk$demand_qty)

# Design capacity proxy in UNITS:
# Convert line machine-hour capacity to units using observed avg machine-hours per unit from raw events.
shift_hours <- c("2x8h"=16, "3x8h"=24, "1x10h"=10)
cap_mh_week <- B$lines %>%
  mutate(
    daily_capacity_machine_hours = if_else(is.na(daily_capacity_machine_hours),
                                           as.numeric(shift_hours[shift_pattern])*0.85,
                                           daily_capacity_machine_hours),
    weekly_capacity_machine_hours = daily_capacity_machine_hours * 6
  ) %>% summarise(total_cap_mh = sum(weekly_capacity_machine_hours)) %>% pull(total_cap_mh)

mh_per_unit <- B$prod %>%
  filter(produced_qty > 0) %>%
  summarise(mh_per_unit = sum(machine_hours, na.rm=TRUE) / sum(produced_qty, na.rm=TRUE)) %>%
  pull(mh_per_unit)

mh_per_unit <- ifelse(is.na(mh_per_unit) || mh_per_unit <= 0, 0.02, mh_per_unit)
design_capacity_units <- cap_mh_week / mh_per_unit

# Heijunka target (level-loaded): choose a stable target that stays within design capacity.
heijunka_target <- min(avg_demand, design_capacity_units * 0.72)

dem_wk <- dem_wk %>%
  mutate(avg_line = avg_demand,
         cap_line = design_capacity_units,
         heijunka = heijunka_target,
         week_ts = as.numeric(as.POSIXct(week)) * 1000)

highchart() %>%
  hc_chart(zoomType="x") %>%
  hc_title(text="Demand per week — variability with capacity context") %>%
  hc_subtitle(text="Ref lines: Avg demand, Potential capacity (unit proxy), Heijunka target (level-loaded)") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis(title=list(text="Unit")) %>%
  hc_add_series(dem_wk, "line", hcaes(x=week_ts, y=demand_qty), name="Demand", color=COL_BEFORE) %>%
  hc_add_series(dem_wk, "line", hcaes(x=week_ts, y=avg_line),  name="Avg demand", dashStyle="ShortDot", color=COL_WARN) %>%
  hc_add_series(dem_wk, "line", hcaes(x=week_ts, y=cap_line),  name="Potetial capacity", dashStyle="Dash", color=COL_BAD) %>%
  hc_add_series(dem_wk, "line", hcaes(x=week_ts, y=heijunka),  name="Heijuka target", dashStyle="ShortDash", color=COL_AFTER) %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE) %>%
  hc_lang_safe(hc_lang)
```
## 7.3 Throughput vs demand gap 
We compare weekly finished throughput (PACK output) against weekly demand due. On average we are not always short, but the gap swing a lot. In the worst shortage week, the gap go down to about `r round(M$gap_min)` unit, and we see `r M$shortage_weeks` week with negative gap. This kind of swing is a common reason for expediting and late shipment, it also push planning to do batching behavior.

```{r throughput-vs-demand}
# Put demand & production on the SAME week type (Date) before joining.
prod_wk <- B$prod %>%
  mutate(week = as.Date(floor_date(step_start_time, "week", week_start=1))) %>%
  group_by(week) %>%
  summarise(produced_qty=sum(produced_qty), .groups="drop")

dem_wk2 <- B$dem %>%
  mutate(week = as.Date(floor_date(due_date, "week", week_start=1))) %>%
  group_by(week) %>%
  summarise(demand_qty = sum(demand_qty), .groups="drop")

td <- full_join(dem_wk2, prod_wk, by="week") %>%
  replace_na(list(demand_qty=0, produced_qty=0)) %>%
  mutate(
    gap = produced_qty - demand_qty,
    week_ts = datetime_to_timestamp(as.POSIXct(week)),
    point_color = case_when(
      gap < 0 ~ COL_BAD,                                        # shortage risk
      gap < quantile(gap, 0.6, na.rm=TRUE) ~ COL_WARN,          # mild overproduction
      TRUE ~ COL_AFTER                                          # heavy overproduction
    )
  ) %>%
  arrange(week)

# Build explicit point objects so Highcharts always renders per-point colors.
gap_pts <- td %>% transmute(x = week_ts, y = gap, color = point_color) %>% list_parse2()

highchart() %>%
  hc_chart(zoomType="xy") %>%
  hc_title(text="Throughput vs Demand") %>%
  hc_subtitle(text="Column = gap (Prod - Demand); lines = produced & demand (zoomable)") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis_multiples(
    list(title=list(text="Unit")),
    list(title=list(text="Gap"), opposite=TRUE)
  ) %>%
  hc_add_series(
    data = gap_pts,
    type="column",
    name="Gap (Prod - Demand)",
    yAxis=1,
    showInLegend=TRUE,
    opacity=0.65
  ) %>%
  hc_add_series(td, type="line", hcaes(x=week_ts, y=produced_qty),
                name="Produced", color=COL_BEFORE, yAxis=0, showInLegend=TRUE) %>%
  hc_add_series(td, type="line", hcaes(x=week_ts, y=demand_qty),
                name="Demand", color="#4285F4", dashStyle="ShortDot", yAxis=0, showInLegend=TRUE) %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE) %>%
  hc_exporting(enabled = TRUE) %>%
  hc_lang_safe(hc_lang)

```
## 7.4 Value stream breakdown (cycle vs waiting)
This chart separate processing time and waiting time by step. The first thing we watch is waiting, because waiting is pure waste, it is time where product not getting value. In the data, the step with the highest average waiting is **`r M$top_wait_step`**, with average waiting around `r round(M$top_wait_avg,1)` minutes, and p95 waiting around `r round(M$top_wait_p95,1)` minutes. This p95(We use p95 to capture the tail risk of waiting time. Mean show normal behavior,p95 is the tail) matters, because it show the ugly tail, when flow is really unstable and batch arrive together.

When we see waiting happening downstream, we usually do not blame that station immediately. Downstream waiting often mean upstream release is not stable, rework loop or batching make work arrive in wave. So the value stream chart is not only a local efficiency chart, it is a flow chart.

```{r vsm-breakdown}
p <- B$prod %>%
  mutate(cycle_min = as.numeric(difftime(step_end_time, step_start_time, units="mins"))) %>%
  arrange(work_order_id, step_start_time) %>%
  group_by(work_order_id) %>%
  mutate(prev_end = lag(step_end_time),
         wait_min = pmax(0, as.numeric(difftime(step_start_time, prev_end, units="mins")))) %>%
  ungroup()

by_step <- p %>%
  group_by(operation_step) %>%
  summarise(
    avg_cycle = mean(cycle_min, na.rm=TRUE),
    p95_cycle = quantile(cycle_min, 0.95, na.rm=TRUE),
    avg_wait  = mean(wait_min, na.rm=TRUE),
    p95_wait  = quantile(wait_min, 0.95, na.rm=TRUE),
    events = n(),
    .groups="drop"
  ) %>%
  arrange(desc(p95_wait))

# Stacked bars (avg cycle + avg wait)
stack_df <- by_step %>%
  select(operation_step, avg_cycle, avg_wait) %>%
  pivot_longer(cols=c(avg_cycle, avg_wait), names_to="component", values_to="minutes") %>%
  mutate(component = recode(component, avg_cycle="Avg cycle", avg_wait="Avg waiting"))

# Keep ONE ordering for both axis and series (avoid label/value mismatch)
order_steps <- by_step$operation_step

highchart() %>%
  hc_chart(type="bar") %>%
  hc_title(text="Value Stream time by operation step") %>%
  hc_subtitle(text="Stacked: Avg cycle vs Avg waiting — waiting is pure waste") %>%
  hc_xAxis(categories = order_steps) %>%
  hc_yAxis(title=list(text="Minutes")) %>%
  hc_plotOptions(series=list(stacking="normal")) %>%
  hc_add_series(name="Avg cycle",  data = round(by_step$avg_cycle, 1), color=COL_BEFORE) %>%
  hc_add_series(name="Avg waiting",data = round(by_step$avg_wait,  1), color=COL_BAD) %>%
  hc_tooltip(shared=TRUE) %>% hc_lang_safe(hc_lang)
```
## 7.5 Bottleneck confirmation (utilization heatmap)
The heatmap show which line is close to full utilization. A line with high utilization for many weeks is a real constraint signal. When constraint has low slack, small variability turn into queue, and queue turn into waiting and WIP. That is why we can see waiting and late delivery even if the factory is not always low throughput. It is constraint variability, not just average speed.

```{r util-heatmap}
shift_hours <- c("2x8h"=16, "3x8h"=24, "1x10h"=10)
lines_cap <- B$lines %>%
  mutate(
    daily_capacity_machine_hours = if_else(is.na(daily_capacity_machine_hours),
                                           as.numeric(shift_hours[shift_pattern])*0.85,
                                           daily_capacity_machine_hours),
    weekly_capacity_machine_hours = daily_capacity_machine_hours * 6
  ) %>% select(production_line_id, weekly_capacity_machine_hours)

util <- B$prod %>%
  mutate(week = format(floor_date(step_start_time, "week", week_start=1), "%Y-%m-%d")) %>%
  group_by(week, production_line_id) %>%
  summarise(machine_hours=sum(machine_hours, na.rm=TRUE), .groups="drop") %>%
  left_join(lines_cap, by="production_line_id") %>%
  mutate(util = machine_hours / weekly_capacity_machine_hours)

# Heatmap data prep for highcharter (x=week index, y=line index)
weeks <- util %>% distinct(week) %>% arrange(week) %>% pull(week)
lines <- util %>% distinct(production_line_id) %>% arrange(production_line_id) %>% pull(production_line_id)

hm <- util %>%
  mutate(x = match(week, weeks) - 1,
         y = match(production_line_id, lines) - 1) %>%
  transmute(x, y, value = round(util, 2))

highchart() %>%
  hc_chart(type="heatmap", zoomType="xy") %>%
  hc_title(text="Bottleneck heatmap — line utilization (machine)") %>%
  hc_xAxis(categories = weeks, title=list(text="Week")) %>%
  hc_yAxis(categories = lines, title=list(text="Line"), reversed=TRUE) %>%
  hc_colorAxis(stops = color_stops(n=5, colors = c("#ecf0f1", "#95a5a6", "#7f8c8d", "#34495e", "#000000"))) %>%
  hc_add_series(
    data = hm %>% list_parse(),
    name = "Utilization",
    borderWidth = 0.5
  ) %>%
  hc_tooltip(formatter = JS("function(){return '<b>Week:</b> '+ this.series.xAxis.categories[this.point.x] + 
                                     '<br><b>Line:</b> '+ this.series.yAxis.categories[this.point.y] +
                                     '<br><b>Util:</b> '+ this.point.value;}")) %>% hc_lang_safe(hc_lang)
```
## 7.6 Quality signal and Pareto defects (quality loss = capacity loss)
Quality loss is not only quality topic, it is capacity topic. In the data, average reject rate is about `r scales::percent(M$reject_avg, accuracy=0.1)`, and average rework rate is about `r scales::percent(M$rework_avg, accuracy=0.1)`. On peak weeks, reject can reach about `r scales::percent(M$reject_peak, accuracy=0.1)`, and rework can reach about `r scales::percent(M$rework_peak, accuracy=0.1)`. These losses reduce effective output, and they also disturb flow because rework create loop.

```{r quality-signal}
# Use consistent definitions:
# - produced = PACK output (real throughput)
# - rejected = QC steps rejects
# - rework   = REWORK step volume (reworked units)

pack_wk <- B$prod %>%
  filter(operation_step == "PACK") %>%
  mutate(week = as.Date(floor_date(step_start_time, "week", week_start=1))) %>%
  group_by(week) %>%
  summarise(produced = sum(produced_qty, na.rm=TRUE), .groups="drop")

qc_wk <- B$prod %>%
  filter(operation_step %in% c("QC_INLINE", "QC_FINAL")) %>%
  mutate(week = as.Date(floor_date(step_start_time, "week", week_start=1))) %>%
  group_by(week) %>%
  summarise(rejected = sum(rejected_qty, na.rm=TRUE), .groups="drop")

rw_wk <- B$prod %>%
  filter(operation_step == "REWORK") %>%
  mutate(week = as.Date(floor_date(step_start_time, "week", week_start=1))) %>%
  group_by(week) %>%
  summarise(rework = sum(if_else(rework_qty>0, rework_qty, produced_qty), na.rm=TRUE), .groups="drop")

q_week <- pack_wk %>%
  full_join(qc_wk, by="week") %>%
  full_join(rw_wk, by="week") %>%
  replace_na(list(produced=0, rejected=0, rework=0)) %>%
  mutate(
    reject_rate = if_else(produced > 0, rejected / produced, NA_real_),
    rework_rate = if_else(produced > 0, rework   / produced, NA_real_),
    week_ts = datetime_to_timestamp(as.POSIXct(week))
  ) %>%
  arrange(week)

highchart() %>%
  hc_chart(zoomType="x") %>%
  hc_title(text="Quality signal — reject & rework rate") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis(title=list(text="Rate"), labels=list(format="{value:.2%}")) %>%
  hc_add_series(q_week, "line", hcaes(x=week_ts, y=reject_rate), name="Reject rate", color=COL_BAD) %>%
  hc_add_series(q_week, "line", hcaes(x=week_ts, y=rework_rate), name="Rework rate", color=COL_WARN, dashStyle="ShortDot") %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE, valueDecimals=3, valueSuffix="") %>%
  hc_lang_safe(hc_lang)
```
The Pareto chart help us see whether defects are systematic. In our data, the top defect is **`r M$top_defect`**, and it takes around `r scales::percent(M$top_defect_share, accuracy=0.1)` of all defects. If we fix top 5 defects, we can potentially recover around `r scales::percent(M$top5_share, accuracy=1)` of defect quantity. This is why Pareto is like a ROI curve for improvement, we fix the big driver first, not everything at once.

```{r defect-pareto}
qi <- B$qi %>% mutate(defect_code = replace_na(defect_code, "NONE"))
def <- qi %>%
  filter(defect_code != "NONE") %>%
  group_by(defect_code) %>%
  summarise(defects = sum(defects_found_qty, na.rm=TRUE), .groups="drop") %>%
  arrange(desc(defects)) %>%
  mutate(pct = defects / sum(defects),
         cum = cumsum(pct))

top_n <- min(10, nrow(def))
def_top <- def %>% slice_head(n=top_n)

highchart() %>%
  hc_title(text="Defect Pareto — top defect type") %>%
  hc_xAxis(categories = def_top$defect_code) %>%
  hc_yAxis_multiples(
    list(title=list(text="Defect qty")),
    list(title=list(text="Cumulative %"), opposite=TRUE, labels=list(format="{value}%"))
  ) %>%
  hc_add_series(name="Defects", type="column", data=def_top$defects, color=COL_BAD) %>%
  hc_add_series(name="Cum%", type="spline", data=round(def_top$cum*100,1), yAxis=1, color=COL_BEFORE) %>%
  hc_tooltip(shared=TRUE) %>% hc_lang_safe(hc_lang)
```
## 7.7 WIP build-up (congestion inside the system)
WIP is work trapped in the system. When WIP rise, it does not mean throughput rise. Often it mean congestion. In the data, average WIP count is about `r round(M$wip_avg,1)` items in progress at mid-day, and peak reach `r M$wip_peak`. We use the WIP-by-step view to see where the congestion sit. If WIP concentrate around certain step, that is where flow is being blocked, usually near the constraint or near rework loop.

```{r wip-build}
# WIP derived from raw events (start/end timestamps).
# Fix legend: series names are operation_step values (no "Series 1/2/3").

prod_ev <- B$prod %>%
  mutate(start = step_start_time, end = step_end_time) %>%
  filter(!is.na(start), !is.na(end), end >= start)

t0 <- as.Date(min(prod_ev$start))
t1 <- as.Date(max(prod_ev$end))
days <- seq.Date(t0, t1, by="day")
steps <- sort(unique(prod_ev$operation_step))

wip_by_step <- purrr::map_dfr(steps, function(st){
  x <- prod_ev %>% filter(operation_step == st)
  cnt <- sapply(days, function(d){
    tt <- as.POSIXct(d) + lubridate::hours(12)  # mid-day sample
    sum(x$start <= tt & x$end > tt)
  })
  tibble(day = days, operation_step = st, wip = as.integer(cnt))
})

# Total WIP
wip_total <- wip_by_step %>%
  group_by(day) %>%
  summarise(wip_total = sum(wip), .groups="drop") %>%
  mutate(day_ts = as.numeric(as.POSIXct(day)) * 1000)

highchart() %>%
  hc_chart(zoomType="x") %>%
  hc_title(text="WIP build-up — total in-progress item") %>%
  hc_subtitle(text="WIP spike = congestion (work waiting inside the system), not throughput") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis(title=list(text="WIP (count)")) %>%
  hc_add_series(wip_total, "line", hcaes(x=day_ts, y=wip_total), name="Total WIP", color=COL_BEFORE) %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE) %>%
  hc_lang_safe(hc_lang)

# WIP by step (stacked area)
wip_step_ts <- wip_by_step %>% mutate(day_ts = as.numeric(as.POSIXct(day)) * 1000)

hc <- highchart() %>%
  hc_chart(type="area", zoomType="x") %>%
  hc_title(text="WIP by operation step — where congestion accumulate") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis(title=list(text="WIP (count)")) %>%
  hc_plotOptions(area=list(stacking="normal", marker=list(enabled=FALSE))) %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE) %>%
  hc_lang_safe(hc_lang)

for (st in steps){
  d <- wip_step_ts %>%
    filter(operation_step == st) %>%
    transmute(x = day_ts, y = wip) %>%
    list_parse2()
  hc <- hc %>% hc_add_series(data=d, type="area", name=st)
}
hc
```
## 7.8 Inventory imbalance (on-hand and backorder)
Inventory snapshot show the pain in a very direct way. In the data, weekly backoder can go up to `r M$backorder_max` unit, and weekly on-hand can go up to `r M$onhand_max` unit. The important Lean pattern is when both on-hand and backoder are high in the same period. We observe this happen in `r M$both_high_n` week in the top quartile range, so the system can build stock but still fail customer. This usually mean poor synchronization, wrong timing, wrong mix, or wrong location allocation.

```{r inventory-imbalance}
inv_wk <- B$inv %>%
  mutate(week = as.Date(floor_date(snapshot_date, "week", week_start=1))) %>%
  group_by(week) %>%
  summarise(
    on_hand_total   = sum(on_hand_qty, na.rm=TRUE),
    backorder_total = sum(backorder_qty, na.rm=TRUE),
    stockout_sku_loc = sum(on_hand_qty <= 0, na.rm=TRUE),
    holding_cost_day = sum(pmax(on_hand_qty,0) * holding_cost_per_unit_per_day, na.rm=TRUE),
    .groups="drop"
  ) %>%
  mutate(week_ts = datetime_to_timestamp(as.POSIXct(week))) %>%
  arrange(week)

highchart() %>%
  hc_chart(zoomType="x") %>%
  hc_title(text="Inventory imbalance  — backorders & stockout signals") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis_multiples(
    list(title=list(text="Unit")),
    list(title=list(text="# sku-locations in stockout"), opposite=TRUE)
  ) %>%
  hc_add_series(inv_wk, "column", hcaes(x=week_ts, y=backorder_total),
                name="Backorder unit", color=COL_BAD, opacity=0.55, showInLegend=TRUE, yAxis=0) %>%
  hc_add_series(inv_wk, "line", hcaes(x=week_ts, y=stockout_sku_loc),
                name="Stockout signals", color=COL_WARN, showInLegend=TRUE, yAxis=1) %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE) %>%
  hc_exporting(enabled = TRUE) %>%
  hc_lang_safe(hc_lang)

# Where is holding cost concentrated? (top 12 sku-location)
inv_top <- B$inv %>%
  mutate(cost_day = pmax(on_hand_qty,0) * holding_cost_per_unit_per_day) %>%
  group_by(location_id, sku_id) %>%
  summarise(cost_day = sum(cost_day, na.rm=TRUE), .groups="drop") %>%
  arrange(desc(cost_day)) %>%
  slice_head(n=12) %>%
  mutate(label = paste0(location_id, " | ", sku_id))

highchart() %>%
  hc_chart(type="bar") %>%
  hc_title(text="Where inventory cost sits  — top sku-location by holding cost/day") %>%
  hc_xAxis(categories = inv_top$label) %>%
  hc_yAxis(title=list(text="Holding cost per day")) %>%
  hc_add_series(name="Holding cost/day", data=round(inv_top$cost_day,2), color=COL_BEFORE) %>%
  hc_tooltip(shared=TRUE) %>%
  hc_lang_safe(hc_lang)
```
## 7.9 Delivery lateness (customer impact)
Delivery lateness is the final output symptom. In the data, about `r scales::percent(M$late_rate, accuracy=1)` of shipments are late. Among late shipments, p95 late days is around `r round(M$late_days_p95,1)` day. This is consistent with a system that has unstable flow and rework loop, because even if total production is not terrible, timing is unstable.

```{r lateness}
sh <- B$sh %>%
  mutate(late_days = pmax(0, as.numeric(difftime(actual_delivery_date, planned_delivery_date, units="days"))))

# show distribution as interactive histogram
bins <- sh %>%
  mutate(bin = pmin(late_days, 15)) %>% # cap long tail
  count(bin)

highchart() %>%
  hc_title(text="Shipment lateness distribution") %>%
  hc_subtitle(text="0 = on time; right tail = expediting / firefighting") %>%
  hc_xAxis(categories = as.character(bins$bin), title=list(text="Late days (capped at 15)")) %>%
  hc_yAxis(title=list(text="Shipments")) %>%
  hc_add_series(name="Shipments", type="column", data=bins$n, color=COL_BEFORE) %>% hc_lang_safe(hc_lang)
```
## 7.10 From diagnosis to solution (Lean first, then Optimization)
At this point we already define the problem with number, demand variability is present, throughput gap swing, waiting and WIP show instability, quality loss steal capacity, inventory show imbalance, and shipment show customer pain. So the solution focus on flow control and remove systematic causes, not only pushing people to work faster.

We start with Lean action around the constraint, controlling release (pull discipline, WIP limit), reducing batching, and making the constraint more stable (setup reduction, standard work). We also focus quality at source using Pareto defects, because less rework mean more stable flow. After dianosing waste and flow problems, we use a weekly LP for production planning. The LP does not replace Lean, it support it. It give a feasible plan under capacity constraint, and it help reduce the mismatch between what we produce and what customer need in timing.

### 7.10.1 Optimization 1 — Production planning LP
```{r production-lp}
prod <- B$prod; lines_m <- B$lines; skus <- B$skus; dem <- B$dem; inv_snap <- B$inv

dem <- dem %>% mutate(week = format(floor_date(due_date, unit="week", week_start=1), "%Y-%m-%d"))
weeks <- sort(unique(dem$week))
sku_list <- sort(unique(skus$sku_id))

demand_wk <- dem %>% group_by(week, sku_id) %>%
  summarise(demand_qty=sum(demand_qty), avg_penalty=mean(penalty_cost_late), .groups="drop")

grid <- expand_grid(week=weeks, sku_id=sku_list) %>%
  left_join(demand_wk, by=c("week","sku_id")) %>%
  mutate(demand_qty = replace_na(demand_qty, 0),
         avg_penalty = replace_na(avg_penalty, median(dem$penalty_cost_late)))

unit_mh <- prod %>% filter(produced_qty > 0) %>%
  group_by(sku_id) %>%
  summarise(mh_per_unit = sum(machine_hours, na.rm=TRUE)/sum(produced_qty, na.rm=TRUE), .groups="drop") %>%
  right_join(skus %>% select(sku_id, product_category), by="sku_id") %>%
  group_by(product_category) %>%
  mutate(mh_per_unit = if_else(is.na(mh_per_unit), median(mh_per_unit, na.rm=TRUE), mh_per_unit)) %>%
  ungroup() %>%
  mutate(mh_per_unit = pmax(0.01, mh_per_unit)) %>%
  select(sku_id, mh_per_unit)

shift_hours <- c("2x8h"=16, "3x8h"=24, "1x10h"=10)
lines_cap <- lines_m %>%
  mutate(daily_capacity_machine_hours = if_else(is.na(daily_capacity_machine_hours),
                                               as.numeric(shift_hours[shift_pattern])*0.85,
                                               daily_capacity_machine_hours),
         weekly_capacity_machine_hours = daily_capacity_machine_hours * 6) %>%
  select(factory_id, production_line_id, weekly_capacity_machine_hours)

line_list <- sort(unique(lines_cap$production_line_id))
cap_map <- lines_cap %>% select(production_line_id, weekly_capacity_machine_hours) %>% deframe()

# eligibility: sku -> factory -> lines
sku_factory_hist <- prod %>% count(sku_id, factory_id, sort=TRUE) %>%
  group_by(sku_id) %>% slice_max(n, n=1, with_ties=FALSE) %>% ungroup() %>%
  select(sku_id, factory_id)
fallback <- skus %>% mutate(factory_id = if_else(product_category %in% c("Tee","Polo","Short","Dress"), "F1", "F2")) %>%
  select(sku_id, factory_id)
sku_factory <- fallback %>% left_join(sku_factory_hist, by="sku_id", suffix=c("_fb","_hist")) %>%
  mutate(factory_id = coalesce(factory_id_hist, factory_id_fb)) %>%
  select(sku_id, factory_id)

eligible <- sku_factory %>%
  left_join(lines_cap %>% select(factory_id, production_line_id), by="factory_id", relationship="many-to-many") %>%
  distinct(sku_id, production_line_id)
elig_set <- eligible %>% mutate(key=paste0(sku_id,"|",production_line_id)) %>% pull(key) %>% unique()

# costs
costs <- inv_snap %>%
  group_by(sku_id) %>%
  summarise(hold_cost_day = mean(holding_cost_per_unit_per_day, na.rm=TRUE),
            stockout_cost = mean(stockout_cost_per_unit, na.rm=TRUE), .groups="drop")
global_hold <- mean(costs$hold_cost_day, na.rm=TRUE)
global_stock <- mean(costs$stockout_cost, na.rm=TRUE)

costs <- tibble(sku_id=sku_list) %>% left_join(costs, by="sku_id") %>%
  mutate(hold_cost_day=replace_na(hold_cost_day, global_hold),
         stockout_cost=replace_na(stockout_cost, global_stock),
         hold_cost_week=hold_cost_day*7) %>%
  select(sku_id, hold_cost_week, stockout_cost)

# initial inventory (WH)
inv0 <- inv_snap %>% filter(location_id %in% c("WH_NORTH","WH_SOUTH")) %>% arrange(snapshot_date)
first_date <- if (nrow(inv0)>0) min(inv0$snapshot_date) else NA
inv0_by_sku <- inv_snap %>%
  filter(location_id %in% c("WH_NORTH","WH_SOUTH"), snapshot_date==first_date) %>%
  group_by(sku_id) %>% summarise(inv0=sum(on_hand_qty), .groups="drop")
inv0_by_sku <- tibble(sku_id=sku_list) %>% left_join(inv0_by_sku, by="sku_id") %>% mutate(inv0=replace_na(inv0,0))

W <- length(weeks); S <- length(sku_list); L <- length(line_list)
n_x <- W*S*L; n_I <- W*S; n_B <- W*S; n_var <- n_x + n_I + n_B
offset_I <- n_x; offset_B <- n_x + n_I

mh_map <- unit_mh %>% deframe()
hold_map <- costs %>% select(sku_id, hold_cost_week) %>% deframe()
stock_map <- costs %>% select(sku_id, stockout_cost) %>% deframe()
penalty_mat <- grid %>% mutate(key=paste0(week,"|",sku_id)) %>% select(key, avg_penalty) %>% deframe()
demand_map  <- grid %>% mutate(key=paste0(week,"|",sku_id)) %>% select(key, demand_qty) %>% deframe()

obj <- rep(0, n_var)
for (w in seq_len(W)){
  for (s in seq_len(S)){
    sku <- sku_list[s]
    obj[offset_I + (w-1)*S + s] <- hold_map[[sku]]
    obj[offset_B + (w-1)*S + s] <- stock_map[[sku]] + penalty_mat[[paste0(weeks[w],"|",sku)]]
  }
}

con_mat <- list(); con_dir <- c(); con_rhs <- c()

# capacity
for (w in seq_len(W)){
  for (l in seq_len(L)){
    row <- rep(0, n_var)
    for (s in seq_len(S)){
      pos <- (w-1)*S*L + (s-1)*L + l
      row[pos] <- mh_map[[sku_list[s]]]
    }
    con_mat[[length(con_mat)+1]] <- row; con_dir <- c(con_dir, "<="); con_rhs <- c(con_rhs, cap_map[[line_list[l]]])
  }
}

# ineligible x -> 0
for (w in seq_len(W)){
  for (s in seq_len(S)){
    for (l in seq_len(L)){
      sku <- sku_list[s]; line <- line_list[l]
      if (!(paste0(sku,"|",line) %in% elig_set)){
        pos <- (w-1)*S*L + (s-1)*L + l
        row <- rep(0, n_var); row[pos] <- 1
        con_mat[[length(con_mat)+1]] <- row; con_dir <- c(con_dir, "<="); con_rhs <- c(con_rhs, 0)
      }
    }
  }
}

# balance
for (w in seq_len(W)){
  for (s in seq_len(S)){
    row <- rep(0, n_var)
    Ipos <- offset_I + (w-1)*S + s
    Bpos <- offset_B + (w-1)*S + s
    row[Ipos] <- 1; row[Bpos] <- -1

    rhs0 <- 0
    if (w > 1){
      Iprev <- offset_I + (w-2)*S + s
      Bprev <- offset_B + (w-2)*S + s
      row[Iprev] <- -1; row[Bprev] <- 1
    } else {
      rhs0 <- inv0_by_sku$inv0[inv0_by_sku$sku_id==sku_list[s]]
    }

    for (l in seq_len(L)){
      x_pos <- (w-1)*S*L + (s-1)*L + l
      row[x_pos] <- row[x_pos] - 1
    }

    dem_qty <- demand_map[[paste0(weeks[w],"|",sku_list[s])]]
    rhs <- if (w==1) (rhs0 - dem_qty) else (0 - dem_qty)

    con_mat[[length(con_mat)+1]] <- row; con_dir <- c(con_dir, "="); con_rhs <- c(con_rhs, rhs)
  }
}
con <- do.call(rbind, con_mat)

sol <- lp(direction="min", objective.in=obj, const.mat=con, const.dir=con_dir, const.rhs=con_rhs, all.int=FALSE)
stopifnot(sol$status==0)

x_sol <- sol$solution[1:n_x]
plan <- expand_grid(w=seq_len(W), s=seq_len(S), l=seq_len(L)) %>%
  mutate(pos=(w-1)*S*L + (s-1)*L + l,
         week=weeks[w],
         sku_id=sku_list[s],
         production_line_id=line_list[l],
         planned_qty=x_sol[pos]) %>%
  filter(planned_qty > 1e-6) %>%
  mutate(planned_qty=round(planned_qty,0))

lp_cost <- tibble(
  total_objective = sol$objval,
  holding_cost = sum(obj[(n_x+1):(n_x+n_I)] * sol$solution[(n_x+1):(n_x+n_I)]),
  shortage_cost = sum(obj[(n_x+n_I+1):n_var] * sol$solution[(n_x+n_I+1):n_var])
)

write_csv(plan, file.path(base_dir, "optimization_outputs_r", "production_plan_weekly_lp.csv"))
write_csv(lp_cost, file.path(base_dir, "optimization_outputs_r", "lp_cost_breakdown.csv"))
```
```{r lp-plot}
# Production LP results: show utilization/load by line with explicit legend (line IDs).
# Fix legend: no generic series labels.

plan_line_mh <- plan %>%
  left_join(unit_mh, by="sku_id") %>%
  mutate(required_mh = planned_qty * mh_per_unit) %>%
  group_by(week, production_line_id) %>%
  summarise(required_mh = sum(required_mh),
            planned_units = sum(planned_qty), .groups="drop") %>%
  mutate(week_dt = ymd(week),
         week_ts = as.numeric(as.POSIXct(week_dt)) * 1000) %>%
  left_join(lines_cap %>% select(production_line_id, weekly_capacity_machine_hours),
            by="production_line_id") %>%
  mutate(util = required_mh / weekly_capacity_machine_hours)

# 1) Utilization by line
hc1 <- highchart() %>%
  hc_chart(zoomType="x") %>%
  hc_title(text="Production LP — weekly utilization by line (machine-hours)") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis(title=list(text="Utilization")) %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE) %>%
  hc_lang_safe(hc_lang)

for (ln in sort(unique(plan_line_mh$production_line_id))){
  d <- plan_line_mh %>%
    filter(production_line_id == ln) %>%
    transmute(x = week_ts, y = util) %>%
    list_parse2()
  hc1 <- hc1 %>% hc_add_series(data=d, type="line", name=ln)
}
hc1

# 2) Load (machine-hours) by line
hc2 <- highchart() %>%
  hc_chart(zoomType="x") %>%
  hc_title(text="Production LP — weekly required machine-hours by line") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis(title=list(text="Machine-hours")) %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE) %>%
  hc_lang_safe(hc_lang)

for (ln in sort(unique(plan_line_mh$production_line_id))){
  d <- plan_line_mh %>%
    filter(production_line_id == ln) %>%
    transmute(x = week_ts, y = required_mh) %>%
    list_parse2()
  hc2 <- hc2 %>% hc_add_series(data=d, type="line", name=ln)
}
hc2
```
### 7.10.2 Optimization 2 — Network flow (WH → DC)
In Section 7.3 we already see week with big positive production gap, so it look like we have enough output. But in reality service can still fail at DC level. This is because the extra units can be the wrong SKU mix, or it can stay in the wrong place (stock sits in WH or in another DC). Also even if the stock is correct, lane capacity can limit how fast we move unit from WH to DC, so demand is “there” but shipment cannot catch up. And one more point, demand is need-now by week, while production is often released in batch, so timing can be wrong even when total volume is high. That is why we use network flow here, to make the allocation and capacity limits visible, not only the total output
```{r network-flow}
lanes <- B$lanes %>%
  filter(from_node %in% c("WH_NORTH","WH_SOUTH"),
         str_detect(to_node, "^DC_")) %>%
  mutate(weekly_capacity = capacity_units_per_day * 6)

city_to_dc <- c("Hanoi"="DC_HN","Hai Phong"="DC_HN","Da Nang"="DC_DN","Nha Trang"="DC_DN","HCMC"="DC_HCMC","Can Tho"="DC_CT")
dem2 <- B$dem %>%
  left_join(B$customers %>% select(customer_id, city), by="customer_id") %>%
  mutate(dc_id = city_to_dc[city],
         week = format(floor_date(due_date, unit="week", week_start=1), "%Y-%m-%d"))

weeks2 <- sort(unique(dem2$week))
dcs <- sort(unique(dem2$dc_id))
origins <- sort(unique(lanes$from_node))

dem_dc <- dem2 %>% group_by(week, dc_id) %>% summarise(demand_units=sum(demand_qty), .groups="drop")

# supply from LP (factory -> WH)
lines2 <- B$lines %>% select(production_line_id, factory_id)
plan_wh <- plan %>%
  left_join(lines2, by="production_line_id") %>%
  mutate(wh = if_else(factory_id=="F1","WH_NORTH","WH_SOUTH")) %>%
  group_by(week, wh) %>%
  summarise(supply_units = sum(planned_qty), .groups="drop") %>%
  mutate(week = as.character(week))

supply_grid <- expand_grid(week=weeks2, wh=origins) %>%
  left_join(plan_wh, by=c("week","wh")) %>%
  mutate(supply_units = replace_na(supply_units, 0))

dem_grid <- expand_grid(week=weeks2, dc_id=dcs) %>%
  left_join(dem_dc, by=c("week","dc_id")) %>%
  mutate(demand_units = replace_na(demand_units, 0))

out_rows <- list()
cost_rows <- list()
BIG_M <- 1e4

for (wk in weeks2){
  supply_wk <- supply_grid %>% filter(week==wk)
  demand_wk <- dem_grid %>% filter(week==wk)

  pairs <- expand_grid(from_node=origins, to_node=dcs) %>%
    mutate(key=paste0(from_node,"|",to_node)) %>%
    left_join(lanes %>% mutate(key=paste0(from_node,"|",to_node)) %>%
                select(key, transport_cost_per_unit, weekly_capacity), by="key")

  n_x <- nrow(pairs)
  n_u <- length(dcs)
  n <- n_x + n_u

  obj <- c((pairs$transport_cost_per_unit %>% replace_na(1e6)), rep(BIG_M, n_u))
  cap_vec <- pairs$weekly_capacity; cap_vec[is.na(cap_vec)] <- 0

  con <- list(); dir <- c(); rhs <- c()

  for (o in origins){
    row <- rep(0,n); row[which(pairs$from_node==o)] <- 1
    con[[length(con)+1]] <- row; dir <- c(dir, "<=")
    rhs <- c(rhs, supply_wk$supply_units[supply_wk$wh==o])
  }

  for (i in seq_along(dcs)){
    dc <- dcs[i]
    row <- rep(0,n)
    row[which(pairs$to_node==dc)] <- 1
    row[n_x+i] <- 1
    con[[length(con)+1]] <- row; dir <- c(dir, ">=")
    rhs <- c(rhs, demand_wk$demand_units[demand_wk$dc_id==dc])
  }

  for (i in seq_len(n_x)){
    row <- rep(0,n); row[i] <- 1
    con[[length(con)+1]] <- row; dir <- c(dir, "<=")
    rhs <- c(rhs, cap_vec[i])
  }

  con_mat <- do.call(rbind, con)
  sol2 <- lp(direction="min", objective.in=obj, const.mat=con_mat, const.dir=dir, const.rhs=rhs, all.int=FALSE)
  stopifnot(sol2$status==0)

  x <- sol2$solution[1:n_x]
  u <- sol2$solution[(n_x+1):n]

  out_rows[[length(out_rows)+1]] <- pairs %>%
    mutate(week=wk, ship_units=x) %>%
    filter(ship_units > 1e-6) %>%
    mutate(ship_units=round(ship_units,0))

  cost_rows[[length(cost_rows)+1]] <- tibble(
    week = wk,
    transport_cost = sum((pairs$transport_cost_per_unit %>% replace_na(1e6)) * x),
    unmet_units = sum(u)
  )
}

flow_cost <- bind_rows(cost_rows) %>% mutate(week_dt = ymd(week))
write_csv(flow_cost, file.path(base_dir, "optimization_outputs_r", "network_flow_cost_summary_weekly.csv"))
```
```{r flow-plot}
flow_cost <- flow_cost %>%
  mutate(week_ts = datetime_to_timestamp(as.POSIXct(week_dt))) %>%
  arrange(week_dt)

highchart() %>%
  hc_chart(zoomType="x") %>%
  hc_title(text="Network Flow — transport cost (weekly)") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis(title=list(text="Cost")) %>%
  hc_add_series(flow_cost, "line",
                hcaes(x=week_ts, y=transport_cost),
                color=COL_BEFORE, name="Cost") %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE) %>%
  hc_exporting(enabled = TRUE) %>%
  hc_lang_safe(hc_lang)

highchart() %>%
  hc_chart(zoomType="x") %>%
  hc_title(text="Network Flow — unmet demand units (weekly)") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis(title=list(text="Unit")) %>%
  hc_add_series(flow_cost, "column",
                hcaes(x=week_ts, y=unmet_units),
                color=COL_BAD, name="Unmet", opacity=0.6) %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE) %>%
  hc_exporting(enabled = TRUE) %>%
  hc_lang_safe(hc_lang)

```
## 7.11 Validation (before vs after impact)
Finally we compare BEFORE vs AFTER using the same measurement logic. After of all chart make us need to know what we fix. In the AFTER scenaria, we expect waiting and WIP to be lower and more stable, reject and rework to reduce, inventory to be less imbalanced, and shipment lateness to improve. The chart below show the direction of change and make the story easy to defend in a discussion.

```{r compare-metrics}
compute_weekly <- function(x){
  prod <- x$prod %>% mutate(
    cycle_min = as.numeric(difftime(step_end_time, step_start_time, units="mins")),
    week = as.Date(floor_date(step_start_time, "week", week_start=1))
  ) %>% arrange(work_order_id, step_start_time) %>%
    group_by(work_order_id) %>%
    mutate(prev_end = lag(step_end_time),
           wait_min = pmax(0, as.numeric(difftime(step_start_time, prev_end, units="mins")))) %>%
    ungroup()

  prod %>% group_by(week) %>%
    summarise(
      produced_qty=sum(produced_qty),
      rejected_qty=sum(rejected_qty),
      rework_qty=sum(if_else(rework_qty>0, rework_qty, produced_qty)),
      avg_wait=mean(wait_min, na.rm=TRUE),
      avg_cycle=mean(cycle_min, na.rm=TRUE),
      p95_cycle=quantile(cycle_min, 0.95, na.rm=TRUE),
      .groups="drop"
    ) %>% mutate(scenario=x$scenario)
}

cmp <- bind_rows(compute_weekly(B), compute_weekly(A)) %>%
  mutate(week_ts = datetime_to_timestamp(as.POSIXct(week))) %>%
  arrange(week)

# Interactive compare charts (use week_ts for datetime axis)
highchart() %>%
  hc_chart(zoomType="x") %>%
  hc_title(text="Impact — Throughput (Before vs After)") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis(title=list(text="Unit")) %>%
  hc_add_series(cmp %>% filter(scenario=="before"), "line",
                hcaes(x=week_ts, y=produced_qty),
                name="before", color=COL_BEFORE) %>%
  hc_add_series(cmp %>% filter(scenario=="after"),  "line",
                hcaes(x=week_ts, y=produced_qty),
                name="after",  color=COL_AFTER) %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE) %>%
  hc_exporting(enabled = TRUE) %>%
  hc_lang_safe(hc_lang)

highchart() %>%
  hc_chart(zoomType="x") %>%
  hc_title(text="Impact — Waiting time (Before vs After)") %>%
  hc_xAxis(type="datetime") %>%
  hc_yAxis(title=list(text="Minutes")) %>%
  hc_add_series(cmp %>% filter(scenario=="before"), "line",
                hcaes(x=week_ts, y=avg_wait),
                name="before", color=COL_BEFORE) %>%
  hc_add_series(cmp %>% filter(scenario=="after"),  "line",
                hcaes(x=week_ts, y=avg_wait),
                name="after",  color=COL_AFTER) %>%
  hc_tooltip(shared=TRUE, crosshairs=TRUE) %>%
  hc_exporting(enabled = TRUE) %>%
  hc_lang_safe(hc_lang)

```
## 7.12 Data trust checks (sanity + relationship)

This is a quick trust check, not a claim of “perfect truth”. We want to show the synthetic data behave like a real operating system, meaning the key wastes are connected in a reasonable way. We build a weekly waste table from the raw record, then we check the correlation matrix between main signal. If WIP tends to move with waiting, and backorder tends to move with late delivery, and quality loss sometimes moves with waiting during stressed weeks, then the data is coherent enough for Lean analysis and optimization.

```{r trust-weekly-waste-table}
wk <- function(d) as.Date(floor_date(d, "week", week_start = 1))

dem_wk <- B$dem %>%
  mutate(week = wk(due_date)) %>%
  group_by(week) %>%
  summarise(demand_units = sum(demand_qty, na.rm=TRUE), .groups="drop")

prod_wk <- B$prod %>%
  mutate(week = wk(step_end_time)) %>%
  group_by(week) %>%
  summarise(produced_units = sum(produced_qty, na.rm=TRUE),
            rejected_units = sum(rejected_qty, na.rm=TRUE),
            rework_units   = sum(if_else(rework_qty>0, rework_qty, produced_qty), na.rm=TRUE),
            machine_hours  = sum(machine_hours, na.rm=TRUE),
            .groups="drop")

gaps <- B$prod %>%
  arrange(work_order_id, step_start_time) %>%
  group_by(work_order_id) %>%
  mutate(prev_end = lag(step_end_time),
         gap_min  = as.numeric(difftime(step_start_time, prev_end, units="mins"))) %>%
  ungroup() %>%
  filter(!is.na(gap_min), gap_min >= 0, gap_min <= 24*60)

wait_wk <- gaps %>%
  mutate(week = wk(step_start_time)) %>%
  group_by(week) %>%
  summarise(avg_wait_min = mean(gap_min, na.rm=TRUE),
            p95_wait_min = as.numeric(quantile(gap_min, 0.95, na.rm=TRUE)),
            .groups="drop")

prod_ev <- B$prod %>% transmute(start = step_start_time, end = step_end_time) %>%
  filter(!is.na(start), !is.na(end), end >= start)

t0 <- as.Date(min(prod_ev$start))
t1 <- as.Date(max(prod_ev$end))
days <- seq.Date(t0, t1, by="day")
wip_day <- tibble(day = days) %>%
  mutate(sample_t = as.POSIXct(day) + hours(12),
         wip_total = purrr::map_int(sample_t, ~sum(prod_ev$start <= .x & prod_ev$end > .x)),
         week = wk(day))

wip_wk <- wip_day %>%
  group_by(week) %>%
  summarise(avg_wip = mean(wip_total, na.rm=TRUE), .groups="drop")

inv_wk2 <- B$inv %>%
  mutate(week = wk(snapshot_date)) %>%
  group_by(week) %>%
  summarise(on_hand_units = sum(on_hand_qty, na.rm=TRUE),
            backorder_units = sum(backorder_qty, na.rm=TRUE),
            .groups="drop")

ship_wk <- B$sh %>%
  mutate(week = wk(actual_delivery_date),
         is_late = actual_delivery_date > planned_delivery_date) %>%
  group_by(week) %>%
  summarise(late_rate = mean(is_late, na.rm=TRUE), .groups="drop")

waste_wk <- dem_wk %>%
  full_join(prod_wk, by="week") %>%
  full_join(wait_wk, by="week") %>%
  full_join(wip_wk, by="week") %>%
  full_join(inv_wk2, by="week") %>%
  full_join(ship_wk, by="week") %>%
  arrange(week) %>%
  mutate(reject_rate = rejected_units / pmax(produced_units, 1),
         rework_rate = rework_units / pmax(produced_units, 1),
         service_gap = demand_units - produced_units)

waste_wk
```

```{r trust-correlation-matrix}
m <- waste_wk %>%
  select(demand_units, produced_units, service_gap,
         avg_wait_min, p95_wait_min,
         avg_wip,
         reject_rate, rework_rate,
         on_hand_units, backorder_units,
         late_rate) %>%
  mutate(across(everything(), ~as.numeric(.))) %>%
  drop_na()

corr <- cor(m, use="pairwise.complete.obs")

corr_df <- as.data.frame(as.table(corr)) %>%
  rename(x = Var1, y = Var2, corr = Freq)

highchart() %>%
  hc_chart(type="heatmap") %>%
  hc_title(text="Waste relationship matrix (correlation) — current scenario") %>%
  hc_xAxis(categories = unique(corr_df$x), title=list(text=NULL)) %>%
  hc_yAxis(categories = unique(corr_df$y), title=list(text=NULL), reversed=TRUE) %>%
  hc_colorAxis(stops = color_stops(10, rev(heat.colors(10)))) %>%
  hc_add_series(corr_df, "heatmap",
                hcaes(x = match(x, unique(corr_df$x)) - 1,
                      y = match(y, unique(corr_df$y)) - 1,
                      value = corr),
                name="corr") %>%
  hc_tooltip(pointFormat = "corr: <b>{point.value:.2f}</b>") %>%
  hc_lang_safe(hc_lang)
```

We also keep MECE thinking for the storyline. Demand vs capacity is the trigger, value stream and WIP cover waiting and inventory inside the flow, quality chart cover defect and rework, utilization covers bottleneck evidence, inventory and delivery cover customer pain. This helps the report stay structured and not repetitive.